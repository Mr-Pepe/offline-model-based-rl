from pathlib import Path
from typing import Callable, Literal, Optional, Type

from pydantic import BaseModel
from torch import nn

from offline_mbrl.utils.envs import HOPPER_MEDIUM_REPLAY_V2


class TrainerConfiguration(BaseModel):
    """Configures the training loop.

    Args:
        env_name (str): The environment name. Must be one of the environments defined in
            :py:mod:`.envs`. Defaults to
            :py:data:`.HOPPER_MEDIUM_REPLAY_V2`.
        seed (int): The seed to use for seeding random number generators in torch,
            numpy, and gym. Defaults to 0.
        online_epochs (int): The number of epochs to train the agent online. The agent
            can interact with the real environment during online training. Defaults to
            100.
        offline_epochs (int): The number of epochs to train the agent offline. The agent
            can not interact with the real environment during offline training. Training
            samples can instead be generated by loading an offline dataset (model-free
            training) or interacting with an environment model (model-based training).
            Defaults to 0.
        steps_per_epoch (int): Number of training loop iterations per epoch. Defaults to
            4,000.
        random_steps (int): How many random actions to perform before using the trained
            policy. Applies to interactions with the real environment and an environment
            model. Defaults to 10,000.
        init_steps (int): The number of steps to wait before training an environmnet
            model (only applies to online-only training because the model gets trained
            on the offline dataset at the beginning of offline training). Also the
            number of steps to wait before starting to update the policy during each
            training loop iteration. This ensures that the replay buffer is sufficiently
            filled. Defaults to 10,000.
        env_interactions_per_step (int): Number of interactions with the real
            environment per step during online training. Defaults to 1.
        agent_updates_per_step (int): Number of policy updates per step. Defaults to 1.
        test_episodes (int): The number of test episodes to simulate after each epoch to
            determine the agent's performance. Defaults to 10.
        render_test_episodes (bool): Whether or not to render the first test episode.
            Defaults to False.
        max_episode_length (int): The maximum number of interactions that can be
            performed in the real environment in a single episode. Defaults to 1,000.
        use_env_model (bool): Whether or not to use a MBPO-style model-based training.
            Defaults to False.
        n_parallel_virtual_rollouts (int): How many interactions with the environment
            model to generate in parallel during each iteration of the training loop.
            Defaults to 50.
        max_virtual_rollout_length (Optional[int]): The maximum episode length when
            interacting with an environment model. Passing None does not limit the
            rollout length, which might be a bad idea for environments without a
            termination function because the predictions will eventually become very
            inaccurate. Defaults to 15.
        train_env_model_every (int): How many steps to wait before retraining the
            environment model on the replay buffer. Defaults to 250.
        train_env_model_from_scratch (int): Whether or not to initialize a new
            environment model and traing it from scratch during every environment model
            training. Defaults to False.
        reset_virtual_buffer_after_env_model_training (bool): Whether or not to clear
            the virtual replay buffer after retraining the environment model. This
            ensures that only data generated by the most recent environment model is in
            the virtual replay buffer. Defaults to False.
        pretrained_env_model_path (Optional[Path]): A path to an existing environment
            model. Defaults to None.
        pretrained_agent_path (Optional[Path]): A path to a pretrained agent. Defaults
            to None.
        n_samples_from_dataset (Optional[int]): The number of samples to load from the
            d4rl offline dataset into the real replay buffer. Pass None to load all
            samples. Defaults to 0.
        real_buffer_size (int): The maximum number of samples that can be stored in the
            real replay buffer (containing samples from interacting with the real
            environment). Defaults to 1,000,000.
        virtual_buffer_size (int): The maximum number of samples that can be stored in
            the virtual replay buffer (containing samples from interacting with an
            environment model). Defaults to 1,000,000.
        save_frequency (int): How often (in terms of gap between epochs) to save the
            agent. Defaults to 1.
        device (str): The device to use for torch tensors. Defaults to "cpu".
    """

    env_name: str = HOPPER_MEDIUM_REPLAY_V2
    seed: int = 0
    online_epochs: int = 100
    offline_epochs: int = 0
    steps_per_epoch: int = 4_000
    random_steps: int = 10_000
    init_steps: int = 10_000
    env_interactions_per_step: int = 1
    agent_updates_per_step: int = 1
    test_episodes: int = 10
    render_test_episodes: bool = False
    max_episode_length: int = 1000
    use_env_model: bool = False
    n_parallel_virtual_rollouts: int = 50
    max_virtual_rollout_length: Optional[int] = 15
    train_env_model_every: int = 250
    train_env_model_from_scratch: bool = False
    reset_virtual_buffer_after_env_model_training: bool = False
    pretrained_env_model_path: Optional[Path] = None
    pretrained_agent_path: Optional[Path] = None
    n_samples_from_dataset: Optional[int] = 0
    real_buffer_size: int = int(1e6)
    virtual_buffer_size: int = int(1e6)
    save_frequency: int = 1
    device: str = "cpu"


class EnvironmentModelConfiguration(BaseModel):
    type: Literal["deterministic", "probabilistic"] = "deterministic"
    n_networks: int = 1
    hidden_layer_sizes: tuple[int, ...] = (200, 200, 200, 200)
    pessimism: float = 0
    ood_threshold: float = -1
    mode: Optional[str] = None
    training_batch_size: int = 256
    training_patience: int = 1
    lr: float = 1e-3
    val_split: float = 0.2
    max_number_of_training_batches: Optional[int] = None
    max_number_of_training_epochs: Optional[int] = None
    preprocessing_function: Optional[Callable] = None
    termination_function: Optional[Callable] = None
    reward_function: Optional[Callable] = None
    obs_bounds_trainable: bool = True
    reward_bounds_trainable: bool = True
    device: str = "cpu"


class AgentConfiguration(BaseModel):
    type: Literal["sac", "bc"] = "sac"
    preprocessing_function: Optional[Callable] = None
    hidden_layer_sizes: tuple[int, ...] = (256, 256)
    activation: Type[nn.Module] = nn.ReLU
    training_batch_size: int = 256
    device: str = "cpu"


class BehavioralCloningConfiguration(AgentConfiguration):
    type: Literal["bc"] = "bc"
    lr: float = 3e-4


class SACConfiguration(AgentConfiguration):
    type: Literal["sac"] = "sac"
    pi_lr: float = 3e-4
    q_lr: float = 3e-4
    gamma: float = 0.99
    alpha: float = 0.2
    polyak: float = 0.995


class EpochLoggerConfiguration(BaseModel):
    """Configures an epoch logger.

    Args:
        output_dir (Optional[Path]): A directory for saving results to. If ``None``,
            defaults to a temp directory of the form
            ``/tmp/experiments/somerandomnumber``. Defaults to None.
        output_filename (str): Name for the tab-separated-value file
            containing metrics logged throughout a training run.
            Defaults to ``progress.txt``.
        experiment_name (Optional[str]): Experiment name. If you run multiple training
            runs and give them all the same ``experiment_name``, the plotter
            will know to group them. (Use case: if you run the same
            hyperparameter configuration with multiple random seeds, you
            should give them all the same ``experiment_name``). Defaults to None.
        env_Name (str): The environment name used in the experiment. Defaults to an
            empty string.
    """

    output_dir: Optional[Path] = None
    output_filename: str = "progress.txt"
    experiment_name: Optional[str] = None
    env_name: str = ""
